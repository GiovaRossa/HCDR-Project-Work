{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f261f0ecfc61d0fe369e1688cd54807deab5b69"
   },
   "source": [
    "# Hyperparameter Tuning with Random Forest: Grid and Random Search\n",
    "\n",
    "In this notebook, we will explore two methods for hyperparameter tuning with a machine learning model -- today we'll consider Random Forest. [In contrast](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) to model __parameters__ which are learned during training, model __hyperparameters__ are set by the data scientist ahead of training and control implementation aspects of the model.\n",
    "\n",
    "These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will __not be__ the best across all datasets. The process of [hyperparameter tuning (also called hyperparameter optimization)](https://en.wikipedia.org/wiki/Hyperparameter_optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - **as measured on a validation dataset** - for a problem. \n",
    "\n",
    "There are several approaches to hyperparameter tuning:\n",
    "\n",
    "1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results. \n",
    "2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!\n",
    "3. __Random search__: set up a grid of hyperparameter values and select _random_ combinations to train the model and score. The number of search iterations is set based on time/resources. \n",
    "4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.\n",
    "\n",
    "(This [Wikipedia Article](https://en.wikipedia.org/wiki/Hyperparameter_optimization) provides a good high-level overview of tuning options with links for more details)\n",
    "\n",
    "In this notebook, we will consider approaches 2 and 3 for a Random Forest Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "For this notebook, we will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset. Also, while this notebook specifically uses random forests, the overall approach can be applied for any machine learning model. \n",
    "\n",
    "To \"test\" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. When we do hyperparameter tuning, it's crucial to __not tune the hyperparameters on the testing data__. We can only use the testing data __a single time__ when we evaluate the final model that has been tuned on the validation data. To actually test our methods from this notebook, we would need to train the best model on all of the training data, make predictions on the actual testing data, and then submit our answers to the competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "c02a14c73390dc52f601eabb60ba8042257036d2"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9dda177d0d5bd48ccbca01b2f359c06d5ade2005"
   },
   "source": [
    "Below we read in the data and separate into a training set of 10000 observations and a \"testing set\" of 6000 observations. After creating the testing set, we cannot do any hyperparameter tuning with it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a42d50c0cf134a1147c35e1aea539b12053d24c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape originale training set: (307511, 199)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307511 entries, 0 to 307510\n",
      "Columns: 199 entries, SK_ID_CURR to TARGET\n",
      "dtypes: float64(145), int64(38), object(16)\n",
      "memory usage: 466.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### Read the training set.\n",
    "data_or = pd.read_csv('./train_bureau_corrs_removed.csv')\n",
    "print(f\"Shape originale training set: {data_or.shape}\")\n",
    "print(data_or.info())\n",
    "\n",
    "\n",
    "# Sample 16000 rows.\n",
    "data = data_or.sample(n = 16000, random_state = 42)\n",
    "\n",
    "# Keep only numeric features (DISABLED!)\n",
    "# data = data.select_dtypes('number')\n",
    "\n",
    "# Separate labels from features\n",
    "labels = data.loc[:, 'TARGET']\n",
    "# features = data.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "features = data.drop(columns = ['TARGET'])\n",
    "\n",
    "\n",
    "# Split the small training set into training and testing data (10000 for training, 6000 for testing)\n",
    "# NOTE the use of stratify...\n",
    "train_features, test_features, train_labels, test_labels =\\\n",
    "    train_test_split(features, labels, test_size = 6000, random_state = 42, stratify = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of the classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione classi train set originale: [0.91927118 0.08072882]\n",
      "[0.9185625 0.0814375]\n",
      "[0.9186 0.0814]\n",
      "[0.9185 0.0815]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distribuzione classi train set originale: {np.histogram(data_or['TARGET'].values, [0,1,2], density = True)[0]}\")\n",
    "print(np.histogram(labels, [0,1,2], density = True)[0])\n",
    "print(np.histogram(train_labels, [0,1,2], density = True)[0])\n",
    "print(np.histogram(test_labels, [0,1,2], density = True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f40aa27cf7927c2b361236e3219f1b9e4c6c5f54"
   },
   "source": [
    "We will also use only the numeric features to reduce the number of dimensions which will help speed up the hyperparameter search. Again, this is something we would not want to do on a real problem, but for demonstration purposes, it will allow us to see the concepts in practice (rather than waiting days/months for the search to finish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ecadc412aea2eba03da9349e07973f1a872ae3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (10000, 198)\n",
      "Training labels shape:  (10000,)\n",
      "Testing features shape:  (6000, 198)\n",
      "Test labels shape:  (6000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training features shape: \", train_features.shape)\n",
    "print(\"Training labels shape: \", train_labels.shape)\n",
    "print(\"Testing features shape: \", test_features.shape)\n",
    "print(\"Test labels shape: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44bcf5c01e1a6adf35e619b5f2e3266a1c9e2eeb"
   },
   "source": [
    "# Cross Validation\n",
    "\n",
    "To evaluate each combination of hyperparameter values, we need to measure its performance in terms of accuracy on a validation set. The hyperparameters __can not be tuned on the testing data__. We can only use the testing data __once__ when we evaluate the final model. The testing data is meant to serve as an estimate of the model performance when deployed on real unseen data, and therefore we do not want to optimize our model to the testing data because that will not give us a fair estimate of the actual performance.\n",
    "\n",
    "The correct approach is therefore to use a **validation set**. However, instead of splitting the valuable training data into a separate training and validation set, we use [KFold cross validation](https://www.youtube.com/watch?v=TIgfjmp-4BA). In addition to preserving training data, this should give us a better estimate of generalization performance on the test set than using a single validation set (since then we are probably overfitting to that validation set). The performance of each set of hyperparameters is determined by Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.\n",
    "\n",
    "In this example, we will use **5-fold cross validation** which means training and testing the model with each set of hyperparameter values 5 times to assess performance. Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have a [large enough training set, we can probably get away with just using a single separate validation set](https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s), but cross validation is a **safer method to avoid overfitting**. \n",
    "\n",
    "To implement KFold cross validation, we will use the scikit cross validation function.\n",
    "\n",
    "### Example of Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2a70db1edb65f88ab6fa06f0639260d12b3e550"
   },
   "source": [
    "We have to pass in a set of hyperparameters to the cross validation, so we will use the default hyperparameters in LightGBM. In the `cv` call, the `num_boost_round` is set to 10,000 (`num_boost_round` is the same as `n_estimators`), but this number won't actually be reached because we are using early stopping. As a reminder, the metric we are using is Receiver Operating Characteristic Area Under the Curve (ROC AUC).\n",
    "\n",
    "The code below carries out both cross validation with 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pull in some functions introduce in the previous classes...\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(app_train, app_test) : \n",
    "    le = LabelEncoder()\n",
    "    le_count = 0\n",
    "\n",
    "    # Iterate through the columns\n",
    "    for col in app_train:\n",
    "        if app_train[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories\n",
    "            set_values = app_train[col].unique()\n",
    "            num_values = len(list(set_values))\n",
    "            if num_values <= 2:\n",
    "                print(f\"{col} will be label encoded! Found {num_values} values: {set_values}\")\n",
    "                # Train on the training data\n",
    "                le.fit(app_train[col])\n",
    "                # Transform both training and testing data\n",
    "                app_train[col] = le.transform(app_train[col])\n",
    "                app_test[col] = le.transform(app_test[col])\n",
    "\n",
    "                # Keep track of how many columns were label encoded\n",
    "                le_count += 1\n",
    "\n",
    "    print('%d columns were label encoded.' % le_count)\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    \n",
    "    return app_train, app_test\n",
    "\n",
    "\n",
    "def one_hot_encode(app_train, app_test) :\n",
    "    \n",
    "    # Let's perform the one-hot encoding of categorical features with > 2 values...\n",
    "    app_train = pd.get_dummies(app_train)\n",
    "    app_test = pd.get_dummies(app_test)\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    \n",
    "    return app_train, app_test\n",
    "\n",
    "\n",
    "def align_train_test(app_train, app_test) :\n",
    "    \n",
    "    # Save target variable in a separate Series...\n",
    "    train_labels = app_train['TARGET']\n",
    "\n",
    "    # Align the training and testing data on columns -- this keeps only the columns present in both dataframes.\n",
    "    app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "\n",
    "    # Add the target column back in.\n",
    "    app_train['TARGET'] = train_labels\n",
    "    \n",
    "    return train_labels, app_train, app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "def cross_val(app_train, app_test, classifier = 'rf', cv = 5, params = {}) :\n",
    "    \n",
    "    # Pick the classifier desired by the user...\n",
    "    log_reg = LogisticRegression\n",
    "    if classifier == 'decision' : log_reg = DecisionTreeClassifier\n",
    "    elif classifier == 'SVC' : log_reg = SVC\n",
    "    elif classifier == 'rf' : log_reg = RandomForestClassifier\n",
    "    elif classifier == 'lgbm' : log_reg = LGBMClassifier\n",
    "    else : pass\n",
    "    \n",
    "    \n",
    "    # Label encoding, one hot encoding, train-test alignment.\n",
    "    app_train, app_test = label_encode(app_train, app_test)\n",
    "    print(f\"Shape train e test dopo label encoding: {app_train.shape} e {app_test.shape}\")\n",
    "    \n",
    "    app_train, app_test = one_hot_encode(app_train, app_test)\n",
    "    print(f\"Shape train e test dopo one hot encoding: {app_train.shape} e {app_test.shape}\")\n",
    "    \n",
    "    train_labels, app_train, app_test = align_train_test(app_train, app_test)\n",
    "    print(f\"Shape train e test dopo allineamento: {app_train.shape} e {app_test.shape}\")\n",
    "    \n",
    "    train = app_train.drop(columns = ['TARGET'], errors = 'ignore')\n",
    "    test = app_test.copy()\n",
    "    \n",
    " \n",
    "    # Setup the pipeline. \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    clf = Pipeline([('imp', SimpleImputer(strategy = 'median')),\n",
    "                    ('sca', MinMaxScaler(feature_range = (0, 1))),\n",
    "                    ('clf', log_reg(**params))],\n",
    "                    verbose = True)\n",
    "    \n",
    "\n",
    "    # Setup the cross validation. \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    search = cross_val_score(estimator = clf, \n",
    "                             X = train, \n",
    "                             y = train_labels,\n",
    "                             cv = cv,\n",
    "                             scoring='roc_auc',\n",
    "                             n_jobs = -1)\n",
    "\n",
    "    \n",
    "    # Compute feature importance (if applicable).\n",
    "    #feat_imp = pd.Series(0, index=train.columns)\n",
    "    #if(classifier in ['rf', 'lgbm']) :\n",
    "    #    feat_imp = pd.Series(search.best_estimator_.named_steps[\"clf\"].feature_importances_, index=train.columns)\n",
    "        \n",
    "    # Return the results.\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score(app_train, app_test, classifier = 'rf', params = {}) :\n",
    "    \n",
    "    # Pick the classifier desired by the user...\n",
    "    log_reg = LogisticRegression\n",
    "    if classifier == 'decision' : log_reg = DecisionTreeClassifier\n",
    "    elif classifier == 'SVC' : log_reg = SVC\n",
    "    elif classifier == 'rf' : log_reg = RandomForestClassifier\n",
    "    elif classifier == 'lgbm' : log_reg = LGBMClassifier\n",
    "    else : pass\n",
    "    \n",
    "    \n",
    "    # Label encoding, one hot encoding, train-test alignment.\n",
    "    app_train, app_test = label_encode(app_train, app_test)\n",
    "    print(f\"Shape train e test dopo label encoding: {app_train.shape} e {app_test.shape}\")\n",
    "    \n",
    "    app_train, app_test = one_hot_encode(app_train, app_test)\n",
    "    print(f\"Shape train e test dopo one hot encoding: {app_train.shape} e {app_test.shape}\")\n",
    "    \n",
    "    train_labels, app_train, app_test = align_train_test(app_train, app_test)\n",
    "    print(f\"Shape train e test dopo allineamento: {app_train.shape} e {app_test.shape}\")\n",
    "    \n",
    "    train = app_train.drop(columns = ['TARGET'], errors = 'ignore')\n",
    "    test = app_test.copy()\n",
    "    \n",
    " \n",
    "    # Setup the pipeline. \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    clf = Pipeline([('imp', SimpleImputer(strategy = 'median')),\n",
    "                    ('sca', MinMaxScaler(feature_range = (0, 1))),\n",
    "                    ('clf', log_reg(**params))],\n",
    "                    verbose = True)\n",
    "    \n",
    "\n",
    "    # Train the model. \n",
    "    clf.fit(train, train_labels)\n",
    "    \n",
    "    \n",
    "    # Compute the predictions\n",
    "    return clf.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "704dce8d25515252ff5510f319eae1bb021703be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (10000, 198)\n",
      "Training labels shape:  (10000,)\n",
      "Testing features shape:  (6000, 198)\n",
      "Test labels shape:  (6000,)\n",
      "(10000, 199) (6000, 198)\n",
      "NAME_CONTRACT_TYPE will be label encoded! Found 2 values: ['Cash loans' 'Revolving loans']\n",
      "CODE_GENDER will be label encoded! Found 2 values: ['F' 'M']\n",
      "FLAG_OWN_CAR will be label encoded! Found 2 values: ['N' 'Y']\n",
      "FLAG_OWN_REALTY will be label encoded! Found 2 values: ['Y' 'N']\n",
      "4 columns were label encoded.\n",
      "Training Features shape:  (10000, 199)\n",
      "Testing Features shape:  (6000, 198)\n",
      "Shape train e test dopo label encoding: (10000, 199) e (6000, 198)\n",
      "Training Features shape:  (10000, 315)\n",
      "Testing Features shape:  (6000, 310)\n",
      "Shape train e test dopo one hot encoding: (10000, 315) e (6000, 310)\n",
      "Shape train e test dopo allineamento: (10000, 311) e (6000, 310)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training features shape: \", train_features.shape)\n",
    "print(\"Training labels shape: \", train_labels.shape)\n",
    "print(\"Testing features shape: \", test_features.shape)\n",
    "print(\"Test labels shape: \", test_labels.shape)\n",
    "\n",
    "\n",
    "app_train = pd.concat([train_features, train_labels], axis = 1)\n",
    "app_test = test_features\n",
    "print(app_train.shape, app_test.shape)\n",
    "\n",
    "\n",
    "# Cross validation with early stopping\n",
    "params_classifier = {\"class_weight\" : \"balanced\",\n",
    "                     'max_depth': 10,\n",
    "                     'n_estimators' : 150,\n",
    "                     'n_jobs' : -1} \n",
    "cv = cross_val(app_train.copy(), \n",
    "               app_test.copy(), \n",
    "               classifier = 'rf', \n",
    "               cv = 5, \n",
    "               params = params_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7af37605a7b294a07146fcba8f7eceb7517f4164"
   },
   "source": [
    "_cross_val_score_ returns a vector of $n$ values (here $n = 5$), where each value indicates the score the classifier got on a specific fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e4f547977aa72be4bd3d8f364aea10e0eff987d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [0.70836524 0.6942167  0.68759414 0.69918946 0.70753529]\n",
      "avg. score: 0.6993801655638473\n",
      "stddev. score: 0.007910067825057831\n"
     ]
    }
   ],
   "source": [
    "print(f\"scores: {cv}\")\n",
    "print(f\"avg. score: {np.mean(cv)}\")\n",
    "print(f\"stddev. score: {np.std(cv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's train the model with the same parameters considered during the CV, and then compute predictions over the small test set to see its accuracy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME_CONTRACT_TYPE will be label encoded! Found 2 values: ['Cash loans' 'Revolving loans']\n",
      "CODE_GENDER will be label encoded! Found 2 values: ['F' 'M']\n",
      "FLAG_OWN_CAR will be label encoded! Found 2 values: ['N' 'Y']\n",
      "FLAG_OWN_REALTY will be label encoded! Found 2 values: ['Y' 'N']\n",
      "4 columns were label encoded.\n",
      "Training Features shape:  (10000, 199)\n",
      "Testing Features shape:  (6000, 198)\n",
      "Shape train e test dopo label encoding: (10000, 199) e (6000, 198)\n",
      "Training Features shape:  (10000, 315)\n",
      "Testing Features shape:  (6000, 310)\n",
      "Shape train e test dopo one hot encoding: (10000, 315) e (6000, 310)\n",
      "Shape train e test dopo allineamento: (10000, 311) e (6000, 310)\n",
      "[Pipeline] ............... (step 1 of 3) Processing imp, total=   0.3s\n",
      "[Pipeline] ............... (step 2 of 3) Processing sca, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.9s\n",
      "Accuracy on the test set: 0.7035046842548404\n"
     ]
    }
   ],
   "source": [
    "params_classifier = {\"class_weight\" : \"balanced\",\n",
    "                     'max_depth': 10,\n",
    "                     'n_estimators' : 150,\n",
    "                     'n_jobs' : -1}\n",
    "\n",
    "predictions = fit_score(app_train.copy(),\\\n",
    "                        app_test.copy(),\\\n",
    "                        classifier = 'rf',\\\n",
    "                        params = params_classifier)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f\"Accuracy on the test set: {roc_auc_score(test_labels, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7cee89068d9f5fc1cdcad0667fef3122f2c37f46"
   },
   "source": [
    "The score we get is very close to the one observed during the CV!\n",
    "\n",
    "Observe that we used the same strategy we employ when training our model on the whole training set..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f493fb15e0ab416d0e848dfb0fd45ef0c0e0ef36"
   },
   "source": [
    "# Hyperparameter Tuning Implementation\n",
    "\n",
    "Now we have the basic framework in place: we will use cross validation to determine the performance of model hyperparameters. The basic strategy for both grid and random search is simple. For each hyperparameter value combination:\n",
    "\n",
    "1. evaluate the cross validation score and record the results along with the hyperparameters. \n",
    "2. Then, at the end of searching, choose the combination of hyperparameters that yielded the highest cross-validation score...\n",
    "3. ...and train the model on all the training data.\n",
    "4. Finally, make predictions on the test data.\n",
    "\n",
    "## Four parts of Hyperparameter tuning\n",
    "\n",
    "It's helpful to think of hyperparameter tuning as having four parts:\n",
    "\n",
    "1. **Objective function**: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize\n",
    "2. **Domain**: the set of hyperparameter values over which we want to search. \n",
    "3. **Algorithm**: method for selecting the next set of hyperparameters to evaluate in the objective function.\n",
    "4. **Results history**: data structure containing each set of hyperparameters and the resulting score from the objective function.\n",
    "\n",
    "**Questions: which parts do we already have in this notebook?**\n",
    "\n",
    "Switching from grid to random search will only require making minor modifications to these four parts. \n",
    "\n",
    "## 1 - Objective Function\n",
    "\n",
    "The objective function takes in hyperparameters and outputs a value representing a score. Traditionally in optimization, this is a score to minimize, but here our score will be the ROC AUC which of course we want to maximize. What occurs in the middle of the objective function will vary according to the problem, but for this problem, we will use cross validation with the specified model hyperparameters to get the cross-validation ROC AUC. This score will then be used to select the best model hyperparameter values. \n",
    "\n",
    "In addition to returning the value to maximize, our objective function will return the hyperparameters and the iteration of the search. These results will let us go back and inspect what occurred during a search. The code below implements a simple objective function which we can use for both grid and random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d904f4e3ffb10d2a9b1748fb0c236101486ede0"
   },
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration, app_train, app_test):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv = cross_val(app_train, app_test, classifier = 'rf', cv = 5, params = {})\n",
    "    \n",
    "    # results to retun\n",
    "    score = np.mean(cv) \n",
    "    print(f\"Score achieved with {hyperparameters}: {score}\")\n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1944462565245ef7399a4905f5319b0341ada833"
   },
   "outputs": [],
   "source": [
    "params = {\"class_weight\" : \"balanced\", \n",
    "          'n_estimators' : 150}\n",
    "\n",
    "score, params, iteration = objective(params, \n",
    "                                     1, \n",
    "                                     app_train.copy(), \n",
    "                                     app_test.copy())\n",
    "\n",
    "print('The cross-validation ROC AUC was {:.5f}.'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e05fa2e22d73408eb284a28f1110649f5f1a97b8"
   },
   "source": [
    "# 2 - Domain\n",
    "\n",
    "The domain, or search space, is all the possible values for all the hyperparameters that we want to search over. For random and grid search, the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter.\n",
    "\n",
    "## Hyperparameters for Random Forest\n",
    "\n",
    "To see which settings we can tune, let's make a model and print it out. You can also refer to the [LightGBM documentation](http://lightgbm.readthedocs.io/en/latest/Parameters.html) for the description of all the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02a083d4b41697c47dcb21c48bc543c05cb4a4e8"
   },
   "outputs": [],
   "source": [
    "# Create a default model\n",
    "model = RandomForestClassifier()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34854c78b2359cee125731d4f8848ca40aa08527"
   },
   "source": [
    "Some of these we do not need to tune such as `silent`, `objective`, `random_state`, and `n_jobs`. However, there are still many hyperparameters to optimize, and we will consider only a subset to tune. \n",
    "\n",
    "Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned! \n",
    "\n",
    "If we have prior experience with a model, we might know where the best values for the hyperparameters typically lie, or what a good search space is. However, if we don't have much experience, we can simply define a large search space and hope that the best values are in there somewhere. Typically, when first using a method, I define a wide search space centered around the default values. Then, if I see that some values of hyperparameters tend to work better, I can concentrate the search around those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b91a772cb5b4dba4bc383d396826c97ed99b3943"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [5, 7, 10],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [100, 150],\n",
    "    'class_weight': [\"balanced\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2f490bcf726d04224f092bd05cfb346a88ff25a"
   },
   "source": [
    "# 3 - Algorithm for selecting next values\n",
    "\n",
    "Although we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. \n",
    "\n",
    "We will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b70d951ac6d66dcb5d39e0d18e82ad83e58652a"
   },
   "source": [
    "# Results History\n",
    "\n",
    "The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate. Random and grid search are _uninformed_ methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! \n",
    "\n",
    "A dataframe is a useful data structure to hold the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb3d66b0d87aff8da433329682988d531415b5ee"
   },
   "outputs": [],
   "source": [
    "MAX_EVALS = 5\n",
    "\n",
    "# Dataframes for random and grid search\n",
    "random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "print(grid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b12a7681449e0f3e8ed93581fa14dd893ad3d105"
   },
   "source": [
    "# Grid Search Implementation\n",
    "\n",
    "Grid search is best described as exhuastive guess and check. We have a problem: find the hyperparameters that result in the best cross validation score, and a set of values to try in the hyperparameter grid - the domain. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is  in the grid (in reality, we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run).\n",
    "\n",
    "Grid search suffers from one limiting problem: it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid! Let's see how many total hyperparameter settings there are in our simple little grid we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59bfa6c52a24f259a75cfc616f1b6d1f360ecbbd"
   },
   "outputs": [],
   "source": [
    "com = 1\n",
    "for x in param_grid.values():\n",
    "    com *= len(x)\n",
    "print('There are {} combinations'.format(com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c05ac62ed102daa76ebafd4bc8030bf33ed03a1e"
   },
   "source": [
    "Let's assume 10 seconds per evaluation and see how many time evaluating the above combinations would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e3faab45df397ba9470f961d238e71e396d64ec"
   },
   "outputs": [],
   "source": [
    "print('This would take {:.0f} seconds to finish.'.format((10 * com)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3d513c3688b8238aac5ce2abbfc6244b4443ca1"
   },
   "source": [
    "I think we're going to need a better approach! Before we discuss alternatives, let's walk through how we would actually use this grid and evaluate all the hyperparameters.\n",
    "\n",
    "The code below shows the \"algorithm\" for grid search. First, we [unpack the values](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/) in the hyperparameter grid (which is a Python dictionary) using the line `keys, values = zip(*param_grid.items())`.  The key line is `for v in itertools.product(*values)` where we iterate through all the possible combinations of values in the hyperparameter grid one at a time.  For each combination of values, we create a dictionary `hyperparameters = dict(zip(keys, v))` and then pass these to the objective function defined earlier. The objective function returns the cross validation score from the hyperparameters which we record in the dataframe. This process is repeated for each and every combination of hyperparameter values. By using `itertools.product` (from [this Stack Overflow Question and Answer](https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists)), we create a [generator](http://book.pythontips.com/en/latest/generators.html) rather than allocating a list of all possible combinations which would be far too large to hold in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc053e847d75e461d52557aea11d062cedfddb61"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(app_train, app_test, param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        print(f\"Evaluating the following configuration: {hyperparameters}\")\n",
    "                \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i, app_train.copy(), app_test.copy())\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a075d877eadd196d2b9b41a46b46b7178b153a10"
   },
   "source": [
    "Normally, in grid search, we do not limit the number of evaluations. The number of evaluations is set by the total combinations in the hyperparameter grid (or the number of years we are willing to wait!). So the lines \n",
    "\n",
    "```\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "```\n",
    "\n",
    "would not be used in actual grid search. Here we will run grid search for 5 iterations just as an example. The results returned will show us the validation score (ROC AUC), the hyperparameters, and the iteration sorted by best performing combination of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6672b3dd8c4047bc16a1bca70a3217bf3b761aa5"
   },
   "outputs": [],
   "source": [
    "grid_results = grid_search(app_train.copy(), app_test.copy(), param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ea3977c21fa859064f19b1dbec0cbdc465b4045"
   },
   "source": [
    "Now, since we have the best hyperparameters, we can evaluate them on our \"test\" data (remember not the real test data)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bff132b94401f3bf83428b9b1af57c39e32486e4"
   },
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "grid_search_params = grid_results.loc[0, 'params']\n",
    "\n",
    "# Train over train data, then compute predictions over the test set.\n",
    "print(f\"Computing predictions with {grid_search_params}\")\n",
    "predictions = fit_score(app_train.copy(),\\\n",
    "                        app_test.copy(),\\\n",
    "                        classifier = 'rf',\\\n",
    "                        params = grid_search_params)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f\"Accuracy on the test set: {roc_auc_score(test_labels, predictions)}\")\n",
    "\n",
    "\n",
    "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4df7aa248aed1e32029256a6425758ce3ad34468"
   },
   "source": [
    "It's interesting that the model scores better on the test set than in cross validation. Usually the opposite happens (higher on cross validation than on test) because the model is tuned to the validation data. In this case, the better performance is probably due to small size of the test data and we get very lucky (although this probably does not translate to the actual competition data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "91dd78307ef1ff64ba1550312276ff02f9131d55"
   },
   "source": [
    "To get a sense of how grid search works, we can look at the progression of hyperparameters that were evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7ebf4e12d2c8359271fa185ba5d7236ded4ae08"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "grid_results['params'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dcffbc9de69cc1bd65a4c016820577881865555"
   },
   "source": [
    "This is grid search trying every single value in the grid! No matter how small the increment between subsequent values of a hyperparameter, it will try them all. Clearly, we are going to need a more efficient approach if we want to find better hyperparameters in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ada1510ec1ed54a9fee50997c2093ab16261ba0"
   },
   "source": [
    "#### Application\n",
    "\n",
    "If you want to run this on the entire dataset feel free to take these functions and put them in a script. However, I would advise against using grid search unless you have a very small hyperparameter grid because this is such as exhaustive method! \n",
    "Later, we will look at results from 1000 iterations of grid and random search run on the same small subset of data as we used above. I have not tried to run any form of grid search on the full data (and probably will not try this method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88f78cd29ffc2a94806acd37876c5c7ad1323f39"
   },
   "source": [
    "# Random Search\n",
    "\n",
    "Random search is surprisingly efficient compared to grid search. Although grid search will find the optimal value of hyperparameters (assuming they are in your grid) eventually, random search will usually find a \"close-enough\" value in far fewer iterations. [This great paper explains why this is so](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf): grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid. Random search in contrast, does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations. \n",
    "\n",
    "As [this article](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881) lays out, random search should probably be the first hyperparameter optimization method tried because of its effectiveness. Even though it's an _uninformed_ method (meaning it does not rely on past evaluation results), random search can still usually find better values than the default and is simple to run.\n",
    "\n",
    "Random search can also be thought of as an algorithm: randomly select the next set of hyperparameters from the grid! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows (again accounting for subsampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5679964bab15104f395235d95fa0259baa01d488"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(50)\n",
    "\n",
    "for k, v in param_grid.items() :\n",
    "    print(k,v)\n",
    "\n",
    "# Randomly sample from dictionary\n",
    "random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "random_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "752973a68ca08e060dc40ea1bb08a904580b37ba"
   },
   "source": [
    "Next, we define the `random_search` function. This takes the same general structure as `grid_search` except for the method used to select the next hyperparameter values. Moreover, random search is always run with a limit on the number of search iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92287f56a80bd35a9cd8e31a91e417fcc344a97f"
   },
   "outputs": [],
   "source": [
    "def random_search(app_train, app_test, param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'], index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        print(f\"Evaluating the following configuration: {hyperparameters}\")\n",
    "                \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i, app_train.copy(), app_test.copy())\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04a714aac0572eb2a7b718cfc518c3f199d3b368"
   },
   "outputs": [],
   "source": [
    "random_results = random_search(app_train, app_test, param_grid, max_evals = 5)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(random_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c879a16aa5c217b664da41f86f980e30412cdd6"
   },
   "source": [
    "We can also evaluate the best random search model on the \"test\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41b7a4dc042776f7f28c0c8d371f927290e18cd0"
   },
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "random_search_params = random_results.loc[0, 'params']\n",
    "\n",
    "# Train over train data, then compute predictions over the test set.\n",
    "print(f\"Computing predictions with {grid_search_params}\")\n",
    "predictions = fit_score(app_train.copy(),\\\n",
    "                        app_test.copy(),\\\n",
    "                        classifier = 'rf',\\\n",
    "                        params = grid_search_params)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f\"Accuracy on the test set: {roc_auc_score(test_labels, predictions)}\")\n",
    "\n",
    "\n",
    "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8b40b3b6a25ed849fa4ef014e71830a76e7c721"
   },
   "source": [
    "Finally, we can view the random search sequence of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3a337e380aea5c07c59663d170f4fced1ee29a2"
   },
   "outputs": [],
   "source": [
    "random_results['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c69590cfed06017904f88dd0752f6746b7a0a517"
   },
   "source": [
    "This time we see hyperparameter values that are all over the place, almost as if they had been selected at random! Random search will do a much better job than grid search of exploring the search domain (for the same number of iterations). If we have a limited time to evaluate hyperparameters, random search is a better option than grid search for exactly this reason.\n",
    "\n",
    "### Stacking Random and Grid Search: some final suggestions\n",
    "\n",
    "One option for a smarter implementation of hyperparameter tuning is to combine random search and grid search: \n",
    "\n",
    "1. Use random search with  a large hyperparameter grid \n",
    "2. Use the results of random search to build a focused hyperparameter grid around the best performing hyperparameter values.\n",
    "3. Run grid search on the reduced hyperparameter grid. \n",
    "4. Repeat grid search on more focused grids until maximum computational/time budget is exceeded.\n",
    "\n",
    "Finally, worth of mention is the Bayesian optimization, which provides the advantages of a random search while attempting to focus on regions where the best combinations of hyperparameters may be located.\n",
    "See also https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going back to the code we've seen in previous classes...\n",
    "\n",
    "We can avoid the need of implementing all the logic behind the grid and random search we've seen in the above cells by using the _GridSearchCV_ and _RandomizedSearchCV_ classes.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "**Remember**: we already used them before! In fact we can reuse that code here. Thus, let's pull in code we've used in previous classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(app_train, app_test) : \n",
    "    le = LabelEncoder()\n",
    "    le_count = 0\n",
    "\n",
    "    # Iterate through the columns\n",
    "    for col in app_train:\n",
    "        if app_train[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories\n",
    "            set_values = app_train[col].unique()\n",
    "            num_values = len(list(set_values))\n",
    "            if num_values <= 2:\n",
    "                print(f\"{col} will be label encoded! Found {num_values} values: {set_values}\")\n",
    "                # Train on the training data\n",
    "                le.fit(app_train[col])\n",
    "                # Transform both training and testing data\n",
    "                app_train[col] = le.transform(app_train[col])\n",
    "                app_test[col] = le.transform(app_test[col])\n",
    "\n",
    "                # Keep track of how many columns were label encoded\n",
    "                le_count += 1\n",
    "\n",
    "    print('%d columns were label encoded.' % le_count)\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    \n",
    "    return app_train, app_test\n",
    "\n",
    "\n",
    "def one_hot_encode(app_train, app_test) :\n",
    "    \n",
    "    # Let's perform the one-hot encoding of categorical features with > 2 values...\n",
    "    app_train = pd.get_dummies(app_train)\n",
    "    app_test = pd.get_dummies(app_test)\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    \n",
    "    return app_train, app_test\n",
    "\n",
    "\n",
    "def align_train_test(app_train, app_test) :\n",
    "    \n",
    "    # Save target variable in a separate Series...\n",
    "    train_labels = app_train['TARGET']\n",
    "\n",
    "    # Align the training and testing data on columns -- this keeps only the columns present in both dataframes.\n",
    "    app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "\n",
    "    # Add the target column back in.\n",
    "    app_train['TARGET'] = train_labels\n",
    "    \n",
    "    return train_labels, app_train, app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "def classify(app_train, app_test, type_search = \"grid\", classifier = 'rf', cv = 5, param_grid = {}, n_jobs = 1) :\n",
    "    \n",
    "    # Pick the classifier desired by the user...\n",
    "    log_reg = LogisticRegression\n",
    "    if classifier == 'decision' : log_reg = DecisionTreeClassifier\n",
    "    elif classifier == 'SVC' : log_reg = SVC\n",
    "    elif classifier == 'rf' : log_reg = RandomForestClassifier\n",
    "    elif classifier == 'lgbm' : log_reg = LGBMClassifier\n",
    "    else : pass\n",
    "    \n",
    "    \n",
    "    # Label encoding, one hot encoding, train-test alignment.\n",
    "    app_train, app_test = label_encode(app_train, app_test)\n",
    "    app_train, app_test = one_hot_encode(app_train, app_test)\n",
    "    train_labels, app_train, app_test = align_train_test(app_train, app_test)\n",
    "    \n",
    "    \n",
    "    train = app_train.drop(columns = ['TARGET'], errors = 'ignore')\n",
    "    test = app_test.copy()\n",
    "    \n",
    " \n",
    "    # Setup the pipeline. \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    clf = Pipeline([('imp', SimpleImputer(strategy = 'median')),\n",
    "                    ('sca', MinMaxScaler(feature_range = (0, 1))),\n",
    "                    ('clf', log_reg())],\n",
    "                    verbose = True)\n",
    "    \n",
    "\n",
    "    # Setup the search: grid or random? Pick your favourite! \n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    search = 0\n",
    "    if type_search == 'grid' :\n",
    "        search = GridSearchCV(estimator = clf, \n",
    "                              param_grid = param_grid, \n",
    "                              cv = cv,\n",
    "                              scoring = 'roc_auc',\n",
    "                              n_jobs = n_jobs,\n",
    "                              verbose = 1)\n",
    "    else :\n",
    "        search = RandomizedSearchCV(estimator = clf, \n",
    "                                    param_distributions = param_grid,\n",
    "                                    cv = cv,\n",
    "                                    scoring = 'roc_auc',\n",
    "                                    n_jobs = n_jobs,\n",
    "                                    n_iter = 3,\n",
    "                                    verbose = 1)\n",
    "    \n",
    "    \n",
    "    # Training ...\n",
    "    search.fit(train, train_labels)\n",
    "\n",
    "    \n",
    "    #print('Miscellanea of results:', search.cv_results_)\n",
    "    #print('Score achieved by the best config. during stratified CV:', search.best_score_)\n",
    "    #print('Best estimator config:', search.best_estimator_)\n",
    "\n",
    "    \n",
    "    # Compute predictions...\n",
    "    log_reg_pred = search.predict_proba(test)[:, 1]\n",
    "    # print(log_reg_pred)\n",
    "\n",
    "    \n",
    "    # Final result dataframe.\n",
    "    submit = app_test[['SK_ID_CURR']].copy()\n",
    "    submit.loc[:, 'TARGET'] = log_reg_pred\n",
    "\n",
    "    \n",
    "    # Compute feature importance (if applicable).\n",
    "    feat_imp = pd.Series(0, index=train.columns)\n",
    "    if(classifier in ['rf', 'lgbm']) :\n",
    "        feat_imp = pd.Series(search.best_estimator_.named_steps[\"clf\"].feature_importances_, index=train.columns)\n",
    "        \n",
    "    # Return the results.\n",
    "    return submit, search.cv_results_, feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'clf__max_leaf_nodes': [7, 10],\n",
    "    'clf__max_depth': [7, 10],\n",
    "    'clf__n_estimators': [100, 150],\n",
    "    'clf__class_weight': [\"balanced\"]\n",
    "}\n",
    "\n",
    "predictions, search_res, feat_imp = classify(app_train.copy(),\n",
    "                                             app_test.copy(),\n",
    "                                             classifier = 'rf',\n",
    "                                             type_search = \"rand\",\n",
    "                                             cv = 3, \n",
    "                                             param_grid = param_grid,\n",
    "                                             n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search_res['params'])\n",
    "print(search_res['mean_test_score'])\n",
    "\n",
    "df_src_res = pd.DataFrame.from_dict(search_res['params'], orient = 'columns')\n",
    "df_src_res['score'] = search_res['mean_test_score']\n",
    "df_src_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3004fc088380f24ad0226defaf12d43faf3a0fcf"
   },
   "source": [
    "## Score versus Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "397ee0a46c4e7efb4e88795e551eb05bfc54a861"
   },
   "source": [
    "As a final plot, we can show the score versus the value of each hyperparameter. We need to keep in mind that the hyperparameters are not changed one at a time, so if there are relationships between the values and the score, they do not mean that particular hyperparameter is influencing the score. However, we might be able to identify values of hyperparameters that seem more promising. Mostly these plots are for my own interest, to see if there are any trends! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97ff38aaa830262afed599a0325276d1c90826ca"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (24, 6))\n",
    "i = 0\n",
    "\n",
    "# Plot of four hyperparameters\n",
    "for i, hyper in enumerate(['clf__n_estimators', 'clf__max_leaf_nodes', 'clf__max_depth']):\n",
    "        df_src_res[hyper] = df_src_res[hyper].astype(float)\n",
    "        # Scatterplot\n",
    "        sns.regplot(hyper, 'score', data = df_src_res, ax = axs[i])\n",
    "        axs[i].scatter(df_src_res[hyper].max(), df_src_res['score'].max(), marker = '*', s = 200, c = 'k')\n",
    "        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper));\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
