{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model and compute predictions\n",
    "\n",
    "In today's class we'll close the loop started in the previous class, and see how we can train some model over the training set, and then use it to compute the probability that some applicant won't be able to repay a loan (with 1 representing the certainty that an applicant won't be able to repay). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5e67831-4751-4f11-8e07-527e3e092671",
    "_uuid": "ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f"
   },
   "source": [
    "## Read in Data \n",
    "\n",
    "First, we read the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv('./input/application_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d077aee0-5271-440e-bc07-6087eab40b74",
    "_uuid": "cbd1c4111df6f07bc0d479b51f50895e728b717a"
   },
   "outputs": [],
   "source": [
    "# Testing data features\n",
    "app_test = pd.read_csv('./input/application_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)\n",
    "app_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebb64e63-6222-4509-a43c-302c6435ce09",
    "_uuid": "8bf057e523b2d99833f6dc9d95fe6141fb4e325a"
   },
   "source": [
    "# Baseline\n",
    "\n",
    "For a naive baseline, we could guess the same value for all examples on the testing set.  We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This  will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition ([random guessing on a classification task will score a 0.5](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)).\n",
    "\n",
    "Since we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression.\n",
    "\n",
    "## Logistic Regression Implementation\n",
    "\n",
    "To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform the label encoding of categorical features with just 2 values...\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(app_train, app_test) : \n",
    "    le = LabelEncoder()\n",
    "    le_count = 0\n",
    "\n",
    "    # Iterate through the columns\n",
    "    for col in app_train:\n",
    "        if app_train[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories\n",
    "            set_values = app_train[col].unique()\n",
    "            num_values = len(list(set_values))\n",
    "            if num_values <= 2:\n",
    "                print(f\"{col} will be label encoded! Found {num_values} values: {set_values}\")\n",
    "                # Train on the training data\n",
    "                le.fit(app_train[col])\n",
    "                # Transform both training and testing data\n",
    "                app_train[col] = le.transform(app_train[col])\n",
    "                app_test[col] = le.transform(app_test[col])\n",
    "\n",
    "                # Keep track of how many columns were label encoded\n",
    "                le_count += 1\n",
    "\n",
    "    print('%d columns were label encoded.' % le_count)\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    \n",
    "    return app_train, app_test\n",
    "\n",
    "\n",
    "def one_hot_encode(app_train, app_test) :\n",
    "    \n",
    "    # Let's perform the one-hot encoding of categorical features with > 2 values...\n",
    "    app_train = pd.get_dummies(app_train)\n",
    "    app_test = pd.get_dummies(app_test)\n",
    "    \n",
    "    return app_train, app_test\n",
    "\n",
    "\n",
    "def align_train_test(app_train, app_test) :\n",
    "    \n",
    "    # Save target variable in a separate Series...\n",
    "    train_labels = app_train['TARGET']\n",
    "\n",
    "    # Align the training and testing data on columns -- this keeps only the columns present in both dataframes.\n",
    "    app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "\n",
    "    # Add the target column back in.\n",
    "    app_train['TARGET'] = train_labels\n",
    "    \n",
    "    return train_labels, app_train, app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform the label encoding of categorical features with <= 2 values...\n",
    "app_train, app_test = label_encode(app_train, app_test)\n",
    "\n",
    "# Let's perform the one-hot encoding of categorical features with > 2 values...\n",
    "app_train, app_test = one_hot_encode(app_train, app_test)\n",
    "\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)\n",
    "print('Training Features shape: ', app_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, app_train, app_test = align_train_test(app_train, app_test)\n",
    "\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60ef8744-ca3a-4810-8439-2835fbfc1833",
    "_uuid": "784ae2f91cf7792702595a9973ba773b2acdec00"
   },
   "outputs": [],
   "source": [
    "# Prepare the training and test sets...\n",
    "\n",
    "# Drop the target from the training data\n",
    "train = app_train.drop(columns = ['TARGET'], errors = 'ignore')\n",
    "    \n",
    "# Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to impute...\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer\n",
    "\n",
    "...and normalize the data.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Median imputation of missing values\n",
    "# NOTE: different strategies can be used. Another one is to replace NaNs with some fixed value.\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature such that its values fall within the interval [0,1].\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit imputer ON the ***training data*** (finds the median for each column...)\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both train and test sets according to the medians found before.\n",
    "display(train)\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on the training data\n",
    "scaler.fit(train)\n",
    "\n",
    "# Scale the columns within the training and test sets.\n",
    "display(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "display(train)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1bcfab25-cc1c-4553-9473-96fcfeb2a61a",
    "_uuid": "364f0835a46f7a7bb7be487b54d92f5ff50ed341"
   },
   "source": [
    "We'll consider the use of [`LogisticRegression`from Scikit-Learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as our first model. The only change we will make from the default model settings is to lower the [regularization parameter](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default `LogisticRegression`, but it still will set a low bar for any future models.\n",
    "\n",
    "Here we use the familiar Scikit-Learn modeling syntax. We **(1)** first **instantiate the model**, then **(2)** we **train the model using `.fit`** and finally **(3) compute predictions** on the testing data using `.predict_proba` -- remember that we want probabilities and not a 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6462ff85-e3b6-4a5f-b95c-9416841413b1",
    "_uuid": "9e8aba9401e8367f9902d710ba49e820294870e1"
   },
   "outputs": [],
   "source": [
    "# Step 1 -- Instantiate an object representing the model. The constructor can accept several parameters.\n",
    "\n",
    "# Use of Logistic Regression...\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Note: here we're specifying the regularization parameter C\n",
    "log_reg = LogisticRegression(C = 0.0001, class_weight = 'balanced')\n",
    "\n",
    "\n",
    "# Use of SVC...\n",
    "# from sklearn.svm import SVC\n",
    "# log_reg = SVC(C = 0.0001, class_weight = 'balanced', max_iter = 10, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -- Train the model on the training data\n",
    "log_reg.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe98191a-da57-4d50-8d56-7d8077fc6c26",
    "_uuid": "0ad71fb750fac4af2845f30b0af73f5817e46101"
   },
   "source": [
    "Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model `predict.proba` method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan won't be repaid, so we will select the second column.\n",
    "\n",
    "The following code makes the predictions and selects the correct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80c77c89-3fa9-4311-b441-412a4fbb1480",
    "_uuid": "2138782ddbfc9a803dc99a938460fc27d15972a9"
   },
   "outputs": [],
   "source": [
    "# Step 3 -- Compute predictions\n",
    "# NOTE: Make sure to select the second column only (i.e., the probability that an applicant won't be able to repay the loan)\n",
    "log_reg_pred = log_reg.predict_proba(test)[:, 1]\n",
    "print(log_reg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a2612e95b13a94a13f679c1754b6c4fb28c332d"
   },
   "source": [
    "Each of the above predictions represent some probability between 0 and 1 that the associated loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky. \n",
    "\n",
    "Once we compute the probabilities, we want them to be stored in a dataframe with the appropriate format. We use the format required by the challenge (as shown in the CSV example `sample_submission.csv`), where there are only two columns: `SK_ID_CURR` and `TARGET`. We will create a dataframe in this format from the test set and the predictions called `submit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09a3d281e4c7ee6820f402e32f31775851113089"
   },
   "outputs": [],
   "source": [
    "# Final result dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77204f15-c3b9-4c67-8d93-173fa3afceaa",
    "_uuid": "fcaf338e52d8f42f119b31d437b516e336e787ec"
   },
   "outputs": [],
   "source": [
    "# Save the submission to a csv file\n",
    "submit.to_csv('log_reg_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11e18bd5c4e75b931f90a22bc6ff84441a13570c"
   },
   "source": [
    "Let's try to submit the file to Kaggle and see which score we get...\n",
    "\n",
    "__The logistic regression baseline should score around 0.671 when submitted.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just for fun**: let's see how our classifier performs on the training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pred = log_reg.predict_proba(train)[:, 1]\n",
    "print(log_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "test_acc = roc_auc_score(y_true = train_labels, \n",
    "                         y_score = log_reg_pred)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: given the performance achieved on the test set, do we have underfitting or overfitting?\n",
    "\n",
    "**Exercise**: try different C values and explore the other parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a different classifier: SV classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually use scikit SV classifier **we don't need to rewrite much code**...in fact, by commenting/uncommenting a few lines of code in the above cells, we'll be able to use a different classifier! Indeed, in scikit all classifiers follow a protocol imposed by a common interface.\n",
    "\n",
    "Note: technically, in scikit each classifier is a class derived from a set of superclasses that define said interface. Thus, when using any scikit classifier we can expect them to offer a set of common methods. Among these, we're especially interested in **fit** and **predict_proba**.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to submit the file to Kaggle and see which score we get...\n",
    "\n",
    "__The SVC baseline should score around 0.60 when submitted.__\n",
    "\n",
    "- **Exercise 1**: what happens if we use a different number of iterations? Or different C values? \n",
    "- **Exercise 2**: try to use the k-NN classifier (or any other suitable classifier of choice) and see how it goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining of the steps behind our classification task...use of scikit pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pay attention to what we've done before, we can see that going from the initial train data to the actual predictions on the test set takes several steps:\n",
    "\n",
    "- Read the training and test sets\n",
    "- Preprocess the training and test set: categorical encoding, imputing, rescaling, feature selection, ...\n",
    "- Fit some model on training set\n",
    "- Computing predictions on test set via the previously trained model\n",
    "\n",
    "What if we rationalize a little bit the code we wrote? Let's use a **pipeline of transforms with a final estimator.**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Underlying idea**: sequentially apply a list of transforms and a final estimator. Let's then use a pipeline.\n",
    "\n",
    "Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement the fit and transform methods. \n",
    "The final estimator only needs to implement fit.\n",
    "\n",
    "Going back to the operations we've seen before, which falls in which category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and test sets\n",
    "app_train = pd.read_csv('./input/application_train.csv')\n",
    "app_test = pd.read_csv('./input/application_test.csv')\n",
    "\n",
    "\n",
    "# Label encoding...\n",
    "app_train, app_test = label_encode(app_train, app_test)\n",
    "\n",
    "# One-hot encoding...\n",
    "app_train, app_test = one_hot_encode(app_train, app_test)\n",
    "\n",
    "# Let's align train and test data...\n",
    "train_labels, app_train, app_test = align_train_test(app_train, app_test)\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)\n",
    "\n",
    "    \n",
    "# Copy training and test data\n",
    "train = app_train.drop(columns = ['TARGET'], errors = 'ignore')\n",
    "test = app_test.copy()\n",
    "\n",
    "display(train)\n",
    "display(train_labels)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median imputation of missing values\n",
    "# imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature such that it falls within [0,1].\n",
    "# scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# here we're specifying the regularization parameter\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "# log_reg = SVC(C = 0.0001, class_weight = 'balanced', max_iter = 10, probability=True)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([('imp', SimpleImputer(strategy = 'median')),\n",
    "                ('sca', MinMaxScaler(feature_range = (0, 1))),\n",
    "                ('clf', log_reg)],\n",
    "                verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we execute the oh, imp, and sca transforms, and finally fit the clf estimator.\n",
    "clf.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: why we don't include label and one hot encoding in the pipeline? Check the scikit documentation to see the limitations that come with having these functionalities within the pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we compute the predictions over the test set.\n",
    "log_reg_pred = clf.predict_proba(test)[:, 1]\n",
    "print(log_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "\n",
    "submit.head()\n",
    "\n",
    "# Save the submission to a csv file\n",
    "submit.to_csv('log_reg_pipe.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: try to use the SVC and k-NN classifiers, and see how they perform..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the hyper-parameters: grid search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually want to try out some model with different combinations of parameters, to find out the combination that yields the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: read and prepare the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and test sets\n",
    "app_train = pd.read_csv('./input/application_train.csv')\n",
    "app_test = pd.read_csv('./input/application_test.csv')\n",
    "\n",
    "\n",
    "# Label encoding...\n",
    "app_train, app_test = label_encode(app_train, app_test)\n",
    "\n",
    "\n",
    "# One-hot encoding...\n",
    "app_train, app_test = one_hot_encode(app_train, app_test)\n",
    "\n",
    "\n",
    "# Let's align train and test data...\n",
    "train_labels, app_train, app_test = align_train_test(app_train, app_test)\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)\n",
    "\n",
    "    \n",
    "# Copy training and test data\n",
    "train = app_train.drop(columns = ['TARGET'], errors = 'ignore')\n",
    "test = app_test.copy()\n",
    "\n",
    "display(train)\n",
    "display(train_labels)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up the pipeline\n",
    "\n",
    "Note that this time we're not passing any parameter to the classifier's constructor...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the classifier of your choice...\n",
    "log_reg = LogisticRegression()\n",
    "# log_reg = SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([('imp', SimpleImputer(strategy = 'median')),\n",
    "                ('sca', MinMaxScaler(feature_range = (0, 1))),\n",
    "                ('clf', log_reg)],\n",
    "                verbose = True)\n",
    "\n",
    "# Set up the grid of hyperparameters we want to explore...\n",
    "param_grid =\\\n",
    "{\n",
    "    'clf__C': [0.0001, 0.0005]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set up the grid search\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to try out our model with different combinations of parameters. The goal is to find out the combination yielding the best results.\n",
    "\n",
    "Here there are several things we need to keep in mind:\n",
    "\n",
    "- in order to evaluate the performance of different configurations, we need to use the notion of **cross validation**. Thus, we need to reserve part of the training set to _\"emulate\"_ the test set.\n",
    "- the type of cross validation should take into consideration that our problem is imbalanced: **stratified k-fold** cross validation.\n",
    "- we need to use the loss function considered by the challenge (AUC).\n",
    "\n",
    "\n",
    "Let's first set up the grid search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "search = GridSearchCV(estimator = clf, \n",
    "                      param_grid = param_grid, \n",
    "                      cv = 5,\n",
    "                      scoring = 'roc_auc',\n",
    "                      n_jobs = 1,\n",
    "                      verbose = 3)\n",
    "\n",
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the best combination of hyperparameters via the grid search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the results we got from the grid search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Miscellanea of results:', search.cv_results_)\n",
    "print()\n",
    "print('Score achieved by the best config. during stratified CV:', search.best_score_)\n",
    "print()\n",
    "print('Best estimator config:', search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: compute the predictions over the test set and see the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object _search_ will compute the predictions, via the method _predict_proba_, **according to the best configuration found during the fit phase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we compute the predictions over the test set with the best configuration of hyperparameters found before.\n",
    "log_reg_pred = search.predict_proba(test)[:, 1]\n",
    "print(log_reg_pred)\n",
    "\n",
    "# Final result dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "submit.head()\n",
    "\n",
    "# Save the submission to a csv file\n",
    "submit.to_csv('log_reg_grid.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we submit the CSV, we should get around **0.711**. This is close to what we observe over the validation sets used during the grid search, and represents a noticeable improvement from the previous result (which was 0.67). Good!\n",
    "\n",
    "In general, there's a trade off between the time and the computational resources we want to spend on tuning, and the accuracy we want achieve.\n",
    "\n",
    "- **Exercise**: experiment the code above with the SV classifier.\n",
    "\n",
    "\n",
    "- **Exercise 2**: during the grid search you can go as far as experimenting with different models. E.g., https://stackoverflow.com/questions/50265993/alternate-different-models-in-pipeline-for-gridsearchcv\n",
    "\n",
    "\n",
    "- **Exercise 3**: there are other types of searches, i.e., random search and Bayesian search. They're way faster than the grid search, although they may return less accurate estimators. See the links below and try them in place of GridSearchCV.\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "    \n",
    "    https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
